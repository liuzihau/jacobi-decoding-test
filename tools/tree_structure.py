from copy import copy
import torch

class Node:
    def __init__(self, val, prob, parent=None, children=None):
        self.val = val
        self.prob = prob
        self.parent = parent
        self.children = children if children is not None else []
        self.level = self.parent.level + 1 if self.parent is not None else 0

        self.route = None
        self.route_prob = self.parent.route_prob * prob if self.parent is not None else self.prob
            
    def add_child(self, node):
        self.children.append(node)
    
    def update_route(self, index):
        self.route = [index] if self.parent is None else self.parent.route + [index]

    def __repr__(self):
        return f"(Node val:{self.val}, parent:{self.parent}, children:{self.children})"

class TreeStructure:
    """
    Given K and J, output the top K probs candidates of the tree
    root: the normal token that generated by previous autoregressive decoding
    K: the total candidates proposed by this tree
    J: the jacobi sequences
    For example:
    We has a K equals 15 tree and a model has 3 jacobi tokens. 
    this model generate a normal next token t0. 
    And then it has the top 3 candidates retrived from first jacobi token jt11, jt12, jt13, paired with probs jp11, jp12, jp13
    And then it has the top 3 candidates retrived from second jacobi token based on all different token jt11, jt12, jt13
    so we get 9 candidates and their probs in the second jacobi token each 3 for each first candidate from first candidate
    and vice versa for the third jacobi token
    Now we calculate the probs for the 27 path and choose the top 15 route and cut off all other routes
    output a sequences that contained all the routes and think about the most efficient structure that once a candidate been verified we can retrive the whole route fast
    
    """
    def __init__(self, root_val):
        self.index_dict = {}
        self.layers = []
        self.root = self.add_node(root_val, 1, None, [])
    
    def add_node(self, val, prob, parent, children):
        node = Node(val, prob, parent, children)
        if parent is not None:
            parent.add_child(node)
        self.update_layers(node)
        return node

    def update_layers(self, node):
        level_idx = node.level
        while len(self.layers) <= level_idx:
            self.layers.append([])
        self.layers[level_idx].append(node)

    def build_tree(self, jacobi_token, jacobi_token_p):
        for no in range(jacobi_token.shape[0]):  # [jacobi nums, top_k]
            for node in self.layers[-1]:
                for kth_token_idx in range(jacobi_token[no].shape[0]):
                    # p = node.route_prob * jacobi_token_p[no, kth_token_idx].detach().cpu().item()
                    self.add_node(jacobi_token[no, kth_token_idx].detach().cpu().item(), jacobi_token_p[no, kth_token_idx].detach().cpu().item(), node, None)
            
    def reset_index_dict(self):
        self.index_dict = {}

    def __len__(self):
        return sum(len(layer) for layer in self.layers)
    
    def __repr__(self):
        s = ""
        for i, layer in enumerate(self.layers):
            s += f"[{i}th layer] ["
            for node in layer:
                parent = node.parent.val if node.parent is not None else None
                s += f"(val:{node.val}, parent:{parent}), "
            s = s[:-2] + "]\n"
        return s

class InputProcessor:
    """
    input:
    J: total jacobi tokens
    S: num_prev_sequences the prev hidden state amount need to mix with the current jacobi token
    JP: pad id for jacobi token place holder
    method:
    build_data:
        input:
        batched tree routes: the routes output from the tree structure
        (or other things you think you need)
        output:
        a [B, L] sequence that has all the tokens we need for next verify and next model forward pass
        for example:
        the route for J = 3 B = 2 K = 3 can be listed like:
        tensor([[   [  10,  101,   11,  109],
                    [  10,  101,   11,  107],
                    [  10,  102,   21,  209]],

                [   [  20,  201, 1011, 2109],
                    [  20,  202, 1021, 2209],
                    [  20,  201, 1011, 2107]]])
            
        the input_ids for the next batch will be [
        [10, JP, JP, JP, 101, JP, JP, JP, 102, JP, JP, JP,   11, JP, JP, JP,   21, JP, JP, JP,  109, JP, JP, JP,  107, JP, JP, JP,  209, JP, JP, JP],
        [20, JP, JP, JP, 201, JP, JP, JP, 202, JP, JP, JP, 1011, JP, JP, JP, 1021, JP, JP, JP, 2109, JP, JP, JP, 2209, JP, JP, JP, 2107, JP, JP, JP]
        ]
        First, the same parent only need to present once
        Second, each node we all need to paste J pad placeholder for jacobi tokens for the next run
        Third, we use attention mask and cache_position to make them "as normal autoregressive decoding"
        so attention mask for first batch looks like:
        [
          10[0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         101[0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         102[0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
          11[0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
          21[0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         109[0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         107[0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf, -inf, -inf, -inf, -inf],
            [0, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0, -inf, -inf, -inf, -inf],
         209[0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0, -inf, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0, -inf],
            [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,    0,    0,    0,    0]
        ] 
        and the second one might be the same, idk, check it for me.
        cache_position would be:
        [   
            [0, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6],
            [0, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 6, 3, 4, 5, 6, 3, 4, 5, 6]
        ]
        Since JP is predict next J tokens and we also has our speculative tokens need to be verified

        We also has a loss_mask to record all JP position
        [   
            [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],
            [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],
        ]

        We also has a jacobi_indices directly give all the index of our jacobi tokens
        (   
            [0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 1, 1, 1, 1, 1, 1, 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],
            [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19, 21, 22, 23, 25, 26, 27, 29, 30, 31, 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19, 21, 22, 23, 25, 26, 27, 29, 30, 31],
        )

        We finally has a cat_indices which has all the indices that need to be cat together with the jacobi tokens and then pass it across adapter
        take we only need cat 1 previous token with the jacobi token as example: 
        (
            [0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 1, 1, 1, 1, 1, 1, 1, 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],
            [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18, 20, 21, 22, 24, 25, 26, 28, 29, 30, 0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18, 20, 21, 22, 24, 25, 26, 28, 29, 30],
        )
    """
    def __init__(self, input_dtype, attention_dtype, loss_mask_dtype, device, jacobi_token_nums, mix_sequences, jacobi_id=0):
        self.jacobi_token_nums = jacobi_token_nums
        self.mix_sequences = mix_sequences
        self.jacobi_id = jacobi_id
        self.input_dtype = input_dtype
        self.attention_dtype = attention_dtype
        self.loss_mask_dtype = loss_mask_dtype
        self.min_dtype = torch.finfo(self.attention_dtype).min
        self.device = device

    def build_inputs_inline_jacobi_token(self, tree):
        length = len(tree) * (1+self.jacobi_token_nums)
        deny_mask = torch.full((length, length), fill_value=self.min_dtype, dtype=self.attention_dtype, device=self.device)
        attention_mask = torch.ones((length, length), dtype=self.attention_dtype, device=self.device)
        diagonal_mask = torch.arange(1+self.jacobi_token_nums, dtype=self.attention_dtype, device=self.device)
        diagonal_mask = diagonal_mask > diagonal_mask.reshape(-1, 1)
        tree.reset_index_dict()
        input_ids = []
        jacobi_indices = []
        cat_indices = []
        loss_mask = []
        index = 0
        for layer in tree.layers:
            for node in layer:
                tree.index_dict[node] = index
                node.update_route(index)

                if node.parent is not None:
                    current_mask = attention_mask[tree.index_dict[node.parent]]
                else:
                    current_mask = attention_mask[0]
                input_ids += [node.val] + [self.jacobi_id] * self.jacobi_token_nums
                loss_mask += [0] + [1] * self.jacobi_token_nums
                attention_mask[index:index+1+self.jacobi_token_nums, :] = current_mask
                attention_mask[index:index+1+self.jacobi_token_nums, index:index+1+self.jacobi_token_nums] = diagonal_mask
                for j_no in range(self.jacobi_token_nums):
                    index += 1

                    x = self.mix_sequences
                    token_no = j_no
                    pointer, counter = 1, -1
                    token_set = []
                    while x >= 0:
                        if token_no >= 0:
                            token_set.append(node.route[-1] + token_no + 1)
                            token_no -= 1
                            x -= 1
                        elif len(node.route) >= pointer:
                            token_set.append(node.route[-pointer])
                            pointer += 1
                            x -= 1
                        else:
                            token_set.append(counter)
                            counter -= 1
                            x -= 1
                        
                    cat_indices.append(token_set)
                    jacobi_indices.append(index)
                index += 1
        cache_position = ((attention_mask - 1)* -1).type(torch.int32).sum(dim=-1) - 1
        attention_mask = (attention_mask * deny_mask).unsqueeze(0)
        input_ids = torch.tensor(input_ids, dtype=self.input_dtype, device=self.device)
        loss_mask = torch.tensor(loss_mask, dtype=self.loss_mask_dtype, device=self.device)
        jacobi_indices = torch.tensor(jacobi_indices, dtype=self.input_dtype, device=self.device)
        cat_indices = torch.tensor(cat_indices, dtype=self.input_dtype, device=self.device)
        return input_ids, attention_mask, loss_mask, cache_position, jacobi_indices, cat_indices

    def build_inputs_split_jacobi_token(self, tree):
        length = len(tree) * (1+self.jacobi_token_nums)
        deny_mask = torch.full((length, length), fill_value=self.min_dtype, dtype=self.attention_dtype, device=self.device)
        attention_mask = torch.ones((length, length), dtype=self.attention_dtype, device=self.device)
        diagonal_mask = torch.arange(1+self.jacobi_token_nums, dtype=self.attention_dtype, device=self.device)
        diagonal_mask = diagonal_mask > diagonal_mask.reshape(-1, 1)
        tree.reset_index_dict()
        input_ids = []
        loss_mask = []
        index = 0
        for layer in tree.layers:
            for node in layer:
                tree.index_dict[node] = index
                node.update_route(index)

                if node.parent is not None:
                    current_mask = attention_mask[tree.index_dict[node.parent]]
                else:
                    current_mask = attention_mask[0]
                input_ids += [node.val] + [self.jacobi_id] * self.jacobi_token_nums
                loss_mask += [0] + [1] * self.jacobi_token_nums
                attention_mask[index:index+1+self.jacobi_token_nums, :] = current_mask
                attention_mask[index:index+1+self.jacobi_token_nums, index:index+1+self.jacobi_token_nums] = diagonal_mask
                index += 1+self.jacobi_token_nums
        
        cache_position = ((attention_mask - 1)* -1).type(torch.int32).sum(dim=-1) - 1
        attention_mask = (attention_mask * deny_mask).unsqueeze(0)
        input_ids = torch.tensor(input_ids, dtype=self.input_dtype, device=self.device)
        loss_mask = torch.tensor(loss_mask, dtype=self.loss_mask_dtype, device=self.device)
        return input_ids, attention_mask, loss_mask, cache_position

def test_tree_structure():
    # Initialize Tree
    tree = TreeStructure(root_val=0)
    assert len(tree) == 1
    assert tree.root.val == 0
    assert tree.root.prob == 1
    assert tree.root.level == 0
    assert tree.root.route_prob == 1

    # Add First Layer Nodes
    node1 = tree.add_node(val=1, prob=0.6, parent=tree.root, children=[])
    node2 = tree.add_node(val=2, prob=0.4, parent=tree.root, children=[])
    assert len(tree) == 3  # root + 2 children
    assert len(tree.layers) == 2  # root (level 0), children (level 1)
    assert tree.root.children == [node1, node2]

    # Check Parent-Child Relationship
    assert node1.parent == tree.root
    assert node2.parent == tree.root
    assert node1.level == 1
    assert node2.level == 1
    assert node1.route_prob == 0.6
    assert node2.route_prob == 0.4

    # Add Second Layer Nodes
    node3 = tree.add_node(val=3, prob=0.7, parent=node1, children=[])
    node4 = tree.add_node(val=4, prob=0.3, parent=node1, children=[])
    assert len(tree) == 5  # root + 4 children
    assert len(tree.layers) == 3  # root (level 0), layer 1, layer 2
    assert node3.level == 2
    assert node4.level == 2
    assert node3.route_prob == 0.42  # 0.6 * 0.7
    assert node4.route_prob == 0.18  # 0.6 * 0.3

    # Validate Dynamic Layer Expansion
    node5 = tree.add_node(val=5, prob=1.0, parent=node4, children=[])
    assert len(tree) == 6
    assert len(tree.layers) == 4  # New layer added dynamically
    assert node5.route_prob == 0.18  # 0.6 * 0.3 * 1.0

    # Validate Mutable Children Fix
    assert node1.children == [node3, node4]
    assert node5.children == []



    # Validate Dynamic Layer Expansion
    node6 = tree.add_node(val=6, prob=1.0, parent=node2, children=[])
    assert len(tree) == 7
    assert len(tree.layers) == 4  # New layer added dynamically
    assert node6.route_prob == 0.4  # 0.6 * 0.3 * 1.0

    # Validate Mutable Children Fix
    assert node2.children == [node6]
    assert node6.children == []

    print("All tests passed!")
    return tree

if __name__ == "__main__":
    a = Node(1, 1)
    index_dict = {}
    index_dict[a] =3
    print(index_dict[a])

    b = [0, 1, 2]
    c = copy(b)
    c[1]= 3
    print(b, c)

    diagonal_mask = torch.arange(1+2, dtype=torch.int32)
    diagonal_mask = diagonal_mask > diagonal_mask.reshape(-1, 1)
    print(diagonal_mask)
    attention_mask = torch.ones((4, 4), dtype=torch.bool)
    print(attention_mask)

    tree = test_tree_structure()

    input_processor = InputProcessor(
        input_dtype=torch.long, attention_dtype=torch.float32, loss_mask_dtype=torch.long, device="cuda", jacobi_token_nums=2, jacobi_id=87)
    input_ids, final_mask, loss_mask = input_processor.build_inputs(tree)
    print(input_ids)
    print(loss_mask)
    print("sttention mask:")
    final_mask /= final_mask.min()
    for mask in final_mask:
        print(mask.type(torch.int16).detach().cpu().tolist())